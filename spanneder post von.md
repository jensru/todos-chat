**[Claudia Martinez Font](https://www.linkedin.com/in/claudiamartinez225/overlay/about-this-profile/)**

[https://medium.com/design-bootcamp/language-as-ai-design-material-2ae7968c37d3](https://medium.com/design-bootcamp/language-as-ai-design-material-2ae7968c37d3)

### Intro-Text

Wenn man den Ansatz verortet, zeigt sich: er baut auf bekannten Mustern auf, geht aber einen Schritt weiter.

- **In Research Papers** zu â€žTask-Oriented Dialogue Systemsâ€œ findet man vergleichbare Bausteine: **Inputs, States, Outputs**. Das ist nah an einem AI-Briefing, bleibt aber meist abstrakt und akademisch.
- **In AI-Agent-Frameworks** ist die Struktur â€ž**Observation â†’ Action â†’ Result**â€œ weit verbreitet. Sie beschreibt zwar den Ablauf von Agenten, blendet aber die **User-Perspektive** aus.
- **In klassischem UX-Design** wiederum sind **Storyboards und User Flows** vÃ¶llig normal. Doch die AI-spezifischen Layer â€“ Kontextfenster, Tokenverbrauch, Agent-Aktionen â€“ fehlen dort bisher.

Der Extended Task Recipe verbindet diese StrÃ¤nge: er behÃ¤lt die VerstÃ¤ndlichkeit von Storyboards bei, integriert aber die technischen Schichten der AI-Interaktion. Dadurch entsteht eine **gemeinsame Sprache fÃ¼r UX, PM und Dev**, die anschlussfÃ¤hig ist und trotzdem neue Standards setzt.

---

## ðŸ“‹ Beispiel: Task Recipe in Markdown

**Task Recipe: Meeting Summary**

```Markdown
# Task Description: Meeting Summary

## Intent
Create a clear, structured meeting summary from a transcript.

## Task Flow
1. Input: User uploads meeting recording (audio/video).
2. Transcription: AI converts speech â†’ text.
3. Extraction: AI identifies key topics, decisions, and action items.
4. Summarization: AI produces a human-readable summary.
5. Output: User receives structured summary + action items.

## Content Model
- **Meeting Title**
- **Date / Time**
- **Participants**
- **Topics Discussed**
- **Key Decisions**
- **Action Items** (with assignee + due date)

## Sequencing
- Step 1 â†’ Always transcribe before extracting topics.
- Step 2 â†’ Extract decisions first, then action items.
- Step 3 â†’ Summarize only after extraction.

## Machine-Readable Instructions
- Output format: JSON
- Keys: `title`, `participants[]`, `topics[]`, `decisions[]`, `action_items[]`
- Constraints:
  - `decisions[]` must be < 5 items
  - `action_items[]` must include `assignee` + `due_date`
  - Language: same as meeting transcript

Wieso RAG Source? 
```

---

ðŸ‘‰ Damit wird klar:

- **Task Flow** beschreibt den Nutzerweg.
- **Content Model** definiert die Struktur der Daten.
- **Sequencing** legt die Reihenfolge der Schritte fest.
- **Maschinenlesbare Instruktionen** Ã¼bersetzen alles in feste Regeln, die AI direkt ausfÃ¼hren kann.

  

## ðŸ”§ Erweiterung des Musters um AI Input & Output

### 1. **AI Input Layer**

Beschreibt **wie** der Nutzer oder das System Eingaben an die AI gibt.

- **ModalitÃ¤ten:** Chat (Text), Voice, evtl. Upload (Datei, Bild), Sensoren
- **Dominanz:** welche Eingabe primÃ¤r ist (z. B. Voice-first, Chat-first)
- **KontextÃ¼bergabe:** welche Daten zusÃ¤tzlich in den Prompt flieÃŸen (User-Profile, History, RAG-Datenbanken, Systemstatus)
- **Constraints:** wie Eingaben begrenzt oder normalisiert werden (z. B. max. LÃ¤nge, Sprache, Meta-Daten-Filter)

---

### 2. **AI Output Layer**

Beschreibt, was die AI zurÃ¼ckliefert â€“ **nicht nur Text**, sondern auch:

- **Agent Actions:** Weiterleitung an Sub-Agenten oder externe Systeme (z. B. Kalender-Eintrag, API-Call)
- **Zwischenergebnisse:** Protokoll von Zwischen-Schritten (Chain-of-Thought light, aber user-freundlich dargestellt)
- **Context Transparency:** Anzeige, was das System gerade â€žverstandenâ€œ hat
    - Aktueller Kontext (z. B. letzte 3 Interaktionen)
    - KontextfenstergrÃ¶ÃŸe / Tokenverbrauch (ggf. visualisiert: â€žâš¡ 18% of memory usedâ€œ)
- **Output-Format:** nicht nur TextwÃ¼ste â†’ strukturierte Visualisierung
    - Tabellen
    - Karten (Cards, Tags)
    - Diagramme oder interaktive Elemente

---

### 3. **Beispiel-Erweiterung eines Recipes**

(Du kannst das als Zusatz-Bereich in dein Markdown-Schema packen)

```Markdown
# Task Recipe: Meeting Summary (Extended)

## AI Input
- **Primary Input:** Voice (meeting audio)
- **Secondary Input:** Chat (clarification questions from user)
- **System Context:**
  - RAG source = userâ€™s Knowledge Base
  - History = last 3 meetings
- **Constraints:** max. 60 min audio, language = detected

## AI Output
- **Agent Actions:**
  - Summarization Agent
  - Action Item Agent â†’ forwards tasks to Todo-App
- **Intermediate Results:**
  - Transcription (visible to user)
  - Topic clustering (hidden but can be revealed)
- **Context Transparency:**
  - Current context: 3 meetings
  - Context window: 32k tokens, 12k used
- **User Output Format:**
  - Card View: [Topic Card], [Decision Card], [Action Item Card]
  - Export Options: JSON, Markdown, PDF

```

---

### 4. **Warum das wichtig ist**

- **UX-Sicht:** Der User bekommt ein GefÃ¼hl fÃ¼r die **Black Box** â†’ mehr Vertrauen.
- **Dev-Sicht:** Klare Schnittstellen (Inputs/Outputs) â†’ einfacher testbar.
- **Team-Sicht:** Standardisiert, wie AI-Interaktionen dokumentiert und geteilt werden â†’ wiederverwendbar.

---

ðŸ‘‰ Du wÃ¼rdest also nicht nur ein **Task Recipe**, sondern ein **AI Interaction Pattern** bauen, das **Input/Output/Agents/Transparenz** abdeckt. Das ist eigentlich die **nÃ¤chste Evolutionsstufe von Design Patterns fÃ¼r AI**.

  

  

## ðŸ“‹ Beispiel: Extended Task Recipe â€“ _Meeting Summary_ (als Storyboard)

```Markdown
# Task Recipe: Meeting Summary (Extended Storyboard)

## Intent
User wants to turn a meeting recording into a clear, structured summary with action items.

---

## Storyboard (User Journey + AI Interactions)

1. **Start**
   - User opens the Meeting Assistant app.
   - **AI Input:** User uploads an audio file or speaks: â€œSummarize this meeting.â€
   - **Fallback:** File upload button (drag & drop), in case no voice/chat is possible.

2. **Transcription Step**
   - **AI Action:** Converts speech to text.
   - **AI Output (Intermediate):** Transcript preview displayed in scrollable text area.
   - **Fallback:** Download raw transcript as `.txt`.

3. **Topic Extraction**
   - **AI Action:** Clusters discussion points into topics.
   - **AI Output:** Visual topic cards with short headlines.
   - **Fallback:** Simple bullet list view.

4. **Decision & Action Item Extraction**
   - **AI Action:** Identifies key decisions + tasks with assignees.
   - **AI Output:**
     - Decisions displayed as check-marked list.
     - Action items displayed as assignable task cards (name + due date).
   - **Fallback:** Table format (CSV export).

5. **Summarization**
   - **AI Action:** Generates final summary.
   - **AI Output:** Collapsible summary text + cards for each section (topics, decisions, actions).
   - **Fallback:** Single-page PDF.

6. **Context Transparency**
   - **System Info Display:**
     - â€œ3 meetings in contextâ€
     - â€œContext window: 32k tokens, 12k usedâ€
   - **Fallback:** Tooltip explanation if system cannot show live numbers.

7. **Final Output**
   - **AI Output Options:**
     - Card view in app
     - Export as Markdown, PDF, JSON
   - **Fallback:** Plain text email summary.

---

## Content Model
- **Meeting Title**
- **Date / Time**
- **Participants**
- **Topics (cards)**
- **Decisions (bullets)**
- **Action Items (cards with assignee/due date)**

---

## Sequencing
1. Transcription â†’ 2. Topics â†’ 3. Decisions & Actions â†’ 4. Summary
Each step produces an **intermediate, user-visible output**.

---

## Machine-Readable Instructions
- Always produce intermediate results.
- Format decisions/actions as `JSON` for export.
- No more than 5 decisions, 10 action items.
- Respect meeting language (donâ€™t translate).

```

---

### ðŸ”‘ Unterschiede zur â€žklassischenâ€œ Task Recipe-Version

- Du erzÃ¤hlst es wie ein **Storyboard** (User + AI wechseln sich ab).
- Jeder Schritt enthÃ¤lt: **AI Input, AI Output, Fallback**.
- Transparenz-Schritte (Kontextfenster, Tokens) sind Teil der UX.
- Outputs sind **visuell gedacht** (Cards, Bullets, Tables) â†’ keine TextwÃ¼ste.

---

ðŸ‘‰ Damit hast du:

- eine **menschliche ErzÃ¤hlstruktur** (gut fÃ¼r Storyboards, Workshops, Kommunikation)
- - die **AI-Ebene** (Input/Output/Agenten) klar integriert
- - Fallbacks in **klassische Interfaces** (Buttons, Tables, Exporte), wenn Sprache oder Kontext nicht ausreichen.

  

  

  

  

## ðŸ”‘ Begriffe

- **Task Flow**
    
    â†’ Die **Ablaufkette der Schritte**, die ein Nutzer durchlÃ¤uft, um eine Aufgabe zu erledigen.
    
    Beispiel: â€žMeeting-Zusammenfassungâ€œ â†’ 1. Meeting hochladen â†’ 2. Transkribieren â†’ 3. Wichtige Punkte extrahieren â†’ 4. Zusammenfassung schreiben.
    
- **Content Model**
    
    â†’ Eine **Struktur fÃ¼r die Inhalte**, die erzeugt oder verarbeitet werden.
    
    Beispiel: Bei einer Meeting-Zusammenfassung kÃ¶nnte das Content Model sein:
    
    - Titel
    - Teilnehmerliste
    - Key Decisions
    - Action Items
- **Sequencing von Schritten**
    
    â†’ Die **Reihenfolge der Arbeitsschritte** (Task Flow in logische Stufen zerlegt).
    
    Wichtig: Sequencing ist die â€žRegieanweisungâ€œ, wann was passiert.
    
    Beispiel: Erst Transkription, dann Themen-Clustering, dann Zusammenfassung.
    
- **Maschinenlesbare Instruktionen**
    
    â†’ Formale **Regeln oder Prompts**, die AI versteht und strikt befolgen kann.
    
    Beispiel: â€žExtrahiere nur Action Items, formatiere sie als Bullet Points, benutze maximal 10 WÃ¶rter pro Item.â€œ
    

  

  

### ðŸ”§ 1. Rollen-Perspektiven explizit einbauen

- **FÃ¼r UX/PM:** Storyboard + User Journey bleiben der Kern.
- **FÃ¼r Dev/AI-Engineers:** Maschinenlesbare Instruktionen, API-Calls, RAG-Sources.
- **FÃ¼r Stakeholder:** Visuelles Storyboard oder einfache Narrative.
    
    ðŸ‘‰ Du kÃ¶nntest in jedem Recipe markieren, _fÃ¼r wen_ welcher Teil relevant ist.
    

---

### ðŸ”§ 2. Fallbacks systematisieren

Du hast schon Voice/Chat vs. haptische UI erwÃ¤hnt. Man kÃ¶nnte das verallgemeinern:

- **Modality fallback** (Voice â†’ Chat â†’ Button)
- **Output fallback** (Visual Cards â†’ Text â†’ CSV)
- **System fallback** (AI failed â†’ menschlicher Support)
    
    ðŸ‘‰ Das gibt deinen Recipes einen klaren â€žResilienzâ€œ-Aspekt.
    

---

### ðŸ”§ 3. Error & Recovery Patterns

Gerade in AI wichtig: was, wenn das Modell Mist baut?

- Pattern fÃ¼r **Korrekturfragen** (â€žMeintest duâ€¦?â€œ)
- Pattern fÃ¼r **User Override** (User kann Output lÃ¶schen/Ã¤ndern)
- Pattern fÃ¼r **System Self-Healing** (neuen Prompt generieren, Kontext nachladen)

---

### ðŸ”§ 4. Kontext-Transparenz als Pflicht

Du hast das schon erwÃ¤hnt (Kontextfenster, Tokenverbrauch). Ich wÃ¼rde daraus eine **eigene Sektion** machen, damit klar ist: jedes Recipe muss auch eine Art _â€žExplain Meâ€œ-Layer_ haben.

ðŸ‘‰ Vorteil: User vertrauen mehr, Devs kÃ¶nnen besser debuggen.

---

### ðŸ”§ 5. Testbarkeit & Metriken

Recipes kÃ¶nnten auch enthalten:

- **Test Cases** (â€žAI soll 3 Action Items erkennenâ€œ).
- **KPIs** (z. B. â€žâ‰¤5% Halluzinationâ€œ, â€žâ‰¥90% Outputs mit Quelleâ€œ).
    
    ðŸ‘‰ Damit wird das Recipe nicht nur ein Konzept, sondern auch eine **Checkliste fÃ¼r QA**.
    

---

### ðŸ”§ 6. Library-Charakter

Wenn du mehrere Recipes hast, lohnt sich:

- **Tagging & Kategorien** (Onboarding, Summarization, Ideation, Support).
- **Composable Patterns**: ein Recipe kann auf einem anderen aufbauen.
    
    ðŸ‘‰ Das wird dann wie ein â€žAI-Pattern-Katalogâ€œ.
    

---

### ðŸ”‘ Fazit

Dein Ansatz ist jetzt schon **ein BrÃ¼ckenschlag zwischen UX-Storyboards und AI-Briefings**. Mit den ErgÃ¤nzungen â€“ **Rollenmarkierung, Fallback-Systematik, Error-Handling, Kontext-Transparenz, Testbarkeit, Bibliothek-Charakter** â€“ kÃ¶nntest du daraus ein **vollwertiges Framework** machen, das sowohl in Projekten als auch in Research-Papers bestehen wÃ¼rde.

  

  

[https://chatgpt.com/c/68d10911-2ce0-832e-9bc0-c7f4447f8347](https://chatgpt.com/c/68d10911-2ce0-832e-9bc0-c7f4447f8347)

  

Meta-Prompt: Generate a Task Vibe-Coding Prompt

You are an AI Product Designer.

Your job is to take a user-provided **task description** (short keyword OR detailed storyboard)

and transform it into a structured **Task Vibe-Coding Prompt**.

---

## Instructions

1. **Understand the input**
    - If the input is only a keyword (e.g. "Expense Report"), first infer a plausible storyboard of user interactions.
    - If the input is already a storyboard, refine and structure it.
2. **Produce the Task Vibe-Coding Prompt** with these sections:

### 1. Intent

- Clear one-sentence goal of the task.

### 2. Storyboard (User Journey + AI Interactions)

- Describe the flow step by step.
- For each step include:
    - **User Action** (default happy path first!)
    - **AI Input** (what the AI receives at this step)
    - **AI Output** (what the AI produces here)
    - **Fallback** (if AI fails or modality is not possible)
- Emphasize that the **first step** is a natural entry point (e.g. â€œpress recordâ€, â€œopen cameraâ€, â€œclick startâ€).
- Each step should produce **intermediate, user-visible outputs**.

### 3. AI Input (System View)

- **Primary Input:** e.g. Voice, Chat, Upload
- **Secondary Input:** optional alternatives
- **System Context:** e.g. RAG sources, user history, profile
- **Constraints:** e.g. max. length, language rules

### 4. AI Output (System View)

- **Agent Actions:** which agents/subsystems are triggered
- **Intermediate Results:** visible or hidden outputs along the way
- **Context Transparency:** what context the system currently uses (context window size, tokens used, sources)
- **User Output Format:** cards, tables, charts, exports (JSON, PDF, Markdown, CSV)

### 5. Content Model

- Core information entities and their structure (e.g. Title, Participants, Action Items).

### 6. Sequencing

- Ordered list of steps that AI and user must follow.

### 7. Machine-Readable Instructions

- Constraints, formats, or rules the AI must respect.

---

## Output Format

Respond only in Markdown.

Follow this structure exactly:

```Markdown
# Task Vibe-Coding Prompt: [Task Name]

## Intent
[One-sentence goal]

## Storyboard (User Journey + AI Interactions)
1. **Step Name**
   - User Action: [...]
   - AI Input: [...]
   - AI Output: [...]
   - Fallback: [...]

[Continue steps as needed]

## AI Input
- Primary Input: [...]
- Secondary Input: [...]
- System Context: [...]
- Constraints: [...]

## AI Output
- Agent Actions: [...]
- Intermediate Results: [...]
- Context Transparency: [...]
- User Output Format: [...]

## Content Model
- [...]

## Sequencing
1. [...]
2. [...]
3. [...]

## Machine-Readable Instructions
- [...]
```